{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "middle-glance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "data_folder = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ignored-stand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data loader\n",
    "def get_data_pipeline(path_to_trajectories, path_to_observations, test_size=0.2):\n",
    "    trajectories = np.load(path_to_trajectories)\n",
    "    observations = np.load(path_to_observations)\n",
    "    train_obs, test_obs, train_true, test_true  = train_test_split(observations, trajectories, test_size=test_size)\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_obs, train_true))\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((test_obs, test_true))\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "binding-symposium",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = get_data_pipeline('{}/trajectories.npy'.format(data_folder), '{}/observations.npy'.format(data_folder), 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fifth-helen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: ((32, 2), (32, 3)), types: (tf.float64, tf.float64)>\n"
     ]
    }
   ],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "confused-keeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(tf.keras.layers.Conv1D(filters, size, strides=2, padding='same',kernel_initializer=initializer, use_bias=False))\n",
    "    if apply_batchnorm:\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "    result.add(tf.keras.layers.LeakyReLU())\n",
    "    return result\n",
    "\n",
    "def upsample(filters, size, apply_dropout=False):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "    tf.keras.layers.Conv1DTranspose(filters, size, strides=2,\n",
    "                                    padding='same',\n",
    "                                    kernel_initializer=initializer,\n",
    "                                    use_bias=False))\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "    if apply_dropout:\n",
    "        result.add(tf.keras.layers.Dropout(0.5))\n",
    "    result.add(tf.keras.layers.ReLU())\n",
    "    return result\n",
    "\n",
    "def generator():\n",
    "    inputs = tf.keras.layers.Input(shape=[32, 2])\n",
    "\n",
    "    down_stack = [\n",
    "    downsample(32, 4, apply_batchnorm=False),  # (batch_size, 16, 32)\n",
    "    downsample(64, 4),  # (batch_size, 8, 64)\n",
    "    #downsample(128, 4),  # (batch_size, 4, 128)\n",
    "    #downsample(256, 4),  # (batch_size, 2, 256)\n",
    "    #downsample(512, 4),  # (batch_size, 1, 512)\n",
    "    ]\n",
    "\n",
    "    up_stack = [\n",
    "    #upsample(512, 4, apply_dropout=True),  # (batch_size, 2, 1024)\n",
    "    #upsample(256, 4, apply_dropout=True),  # (batch_size, 4, 512)\n",
    "    #upsample(128, 4, apply_dropout=True),  # (batch_size, 8, 256)\n",
    "    upsample(64, 4),  # (batch_size, 16, 128)\n",
    "    ]\n",
    "\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    last = tf.keras.layers.Conv1DTranspose(3, 4,\n",
    "                                         strides=2,\n",
    "                                         padding='same',\n",
    "                                         kernel_initializer=initializer,\n",
    "                                         activation='tanh')  # (batch_size, 256, 256, 3)\n",
    "\n",
    "    x = inputs\n",
    "\n",
    "    # Downsampling through the model\n",
    "    skips = []\n",
    "    for down in down_stack:\n",
    "        x = down(x)\n",
    "        skips.append(x)\n",
    "\n",
    "    skips = reversed(skips[:-1])\n",
    "\n",
    "    # Upsampling and establishing the skip connections\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        x = tf.keras.layers.Concatenate()([x, skip])\n",
    "\n",
    "    x = last(x)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "def Discriminator():\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    inp = tf.keras.layers.Input(shape=[32, 3], name='input_signal')\n",
    "    tar = tf.keras.layers.Input(shape=[32, 3], name='target_signal')\n",
    "\n",
    "    x = tf.keras.layers.concatenate([inp, tar])  # (batch_size, 32, channels*2)\n",
    "\n",
    "    down1 = downsample(32, 4, False)(x)  # (batch_size, 16, 32)\n",
    "    down2 = downsample(64, 4)(down1)  # (batch_size, 8, 64)\n",
    "    #down3 = downsample(256, 4)(down2)  # (batch_size, 32, 32, 256)\n",
    "\n",
    "    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down2)  # (batch_size, 10, 64)\n",
    "    conv = tf.keras.layers.Conv1D(128, 4, strides=1,\n",
    "                                kernel_initializer=initializer,\n",
    "                                use_bias=False)(zero_pad1)  # (batch_size, 5, 128)\n",
    "\n",
    "    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
    "    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
    "    zero_pad2 = tf.keras.layers.ZeroPadding1D()(leaky_relu)  # (batch_size, 7, 128)\n",
    "\n",
    "    last = tf.keras.layers.Conv1D(1, 4, strides=1,\n",
    "                                kernel_initializer=initializer)(zero_pad2)  # (batch_size, 3, 1)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inp, tar], outputs=last)\n",
    "\n",
    "def get_simple_model():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "biological-night",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 16, 32)\n"
     ]
    }
   ],
   "source": [
    "down_model = downsample(32, 4)\n",
    "x = list(train)[0][0]\n",
    "x = x[np.newaxis, :, :]\n",
    "down_result = down_model(x)\n",
    "print (down_result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "sonic-little",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 32, 30)\n"
     ]
    }
   ],
   "source": [
    "up_model = upsample(30, 4)\n",
    "up_result = up_model(down_result)\n",
    "print (up_result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aerial-express",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "nn = generator()\n",
    "y = nn(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "mighty-lunch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 32, 2)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_60 (Sequential)      (None, 16, 32)       256         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_61 (Sequential)      (None, 8, 64)        8448        sequential_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_62 (Sequential)      (None, 16, 64)       16640       sequential_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 16, 96)       0           sequential_62[0][0]              \n",
      "                                                                 sequential_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_transpose_30 (Conv1DTran (None, 32, 3)        1155        concatenate_13[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 26,499\n",
      "Trainable params: 26,243\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-andorra",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
